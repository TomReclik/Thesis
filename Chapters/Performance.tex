
\chapter{Evaluation and Results} % Main chapter title

\label{Performance} % For referencing the chapter elsewhere, use \ref{Chapter1} 

In this chapter the performance of the different classifiers will be studied. In section \ref{sec:perfectconditions} the performance of the networks will be evaluated given that the localizer returns sites belonging to one of the classes that the classifier was trained on. In section \ref{sec:Robustness} the networks robustness will be tested given any possible site that the localizer can find. 

For the training of each classifier the dataset an $80-20$ split is applied for training of each network and testing. In order to prevent overfitting, the training set is further split into $80\%$ actually used for the adjustments of the networks weights and $20\%$ for validation. \\
Furthermore the pixel values are transformed from the interval $\{0,\dots,255\}$ to the interval $[-1,1]$.\\


\newpage
\section{First Classifier}

As described in chapter \ref{cha:Architecture} the first classifier filters all inclusions out and passes all other sites to the next classifier. The size of its input window is chosen in such a way that even large inclusions can be seen by the network. $250\times 250$ was chosen as an input size. Furthermore in order to prevent the network from learning a bias towards one class the training data consists of sites containing inclusions and sites not containing inclusions, evenly spread among inclusions and sites not containing inclusions. In order to have a more representative training set, the training data is then also distributed equally among martensite cracking, interface decohesion, and notch effect sites.

\subsection{Architecture}
Due to the small amount of available data optimization of a networks architecture is not promising to increase its accuracy. Therefore three networks were chosen that performed well in the ImageNet challenge with similar window sizes, namely the Xception network \cite{Xception}, the InceptionResNetV2 network \cite{InceptionResNetV2}, and the InceptionV3 network \cite{InceptionV3}. Their accuracies can be seen in table \ref{tab:AccuracyComparisonNetworks}. Since the InceptionV3 network performed best it was chosen for the first classifier.\\

\begin{table}[H]
 \begin{center}
  \begin{tabular}{@{} *5l @{}} \toprule[2pt]
   Network &  \multicolumn{3}{c}{Accuracy}  \\\midrule
    & ex-situ  & in-situ  & all   \\ 
   Xception  & 0.868 & 0.878 & 0.866\\ 
   InceptionResNetV2  & 0.854 & 0.849 & 0.888\\
 \boxit{8.46cm}   InceptionV3 & 0.901 & 0.863 & 0.915 \\ \bottomrule[2pt]

   \label{tab:AccuracyComparisonNetworks}
  \end{tabular}
 \end{center}
 \caption{Accuracy of the networks trained on different training sets evaluated on the available datasets.}
\end{table}

\subsection{Generalization to new Datasets}
While working only with the data recorded ex-situ, the network generalized well on this dataset. But applying it to newly recorded in-situ data, leads to drops in the networks accuracy, as can be seen in table \ref{tab:AccuracyComparisonInception}. By including the in-situ data into the training dataset, the networks performance was increased drastically. This behavior is also shown in figure \ref{fig:Inception_ex_vs_in}.

\begin{table}[H]
 \begin{center}
  \begin{tabular}{@{} *5l @{}} \toprule[2pt]
   Training set &  &Test set&  \\\midrule
    & ex-situ  & in-situ  & all   \\ 
   ex-situ  & 0.90 & 0.55 & 0.71\\ 
   all  & 0.94 & 0.90 & 0.92\\\bottomrule[2pt]

  \end{tabular}
 \end{center}
 \caption{Accuracy of the networks trained on different training sets evaluated on the available datasets.}
 \label{tab:AccuracyComparisonInception}
\end{table}

\begin{figure}
  \includegraphics[width=\linewidth]{Inception_ex_vs_in.pdf}
\caption{Comparison of the precision between networks trained excluding in-situ data and including in-situ data. There is a visible drop of the accuracy once the classifier trained on the ex-situ data is applied to the in-situ data to just above a random classifier. Including in-situ data into the training leads to a performance just above the network trained on ex-situ data evaluated on ex-situ data.}
\label{fig:Inception_ex_vs_in}
\end{figure}

%\subsection{Choosing a Threshold}
%The output of the network can be seen as a probability that a certain damage site belongs to a damage class. By choosing a threshold above which the classifier should classify a given damage site, a trade-off has to be made between the desired accuracy and the classification rate. In figure \ref{fig:InceptionACC_EFF_THETA} the accuracy of the first network together with its classification rate plotted against the threshold can be seen. Requiring the accuracy to be $95\%$ would correspond to a threshold of $\theta = 0.7$ and a classification rate of $92\%$. The remaining $8\%$ of damage sites have to be labeled afterwards by hand. 
%
%
%%Due to the first classifier acting as a filter for the next classifier, it is necessary to minimize the number of falsely classified damage sites. By introducing a threshold for the network to only decide for a damage site to be of a category if the returned probability exceeds it, the number of falsely classified damage sites can be minimized, at the cost of a lower efficiency. By labeling the not classified damage sites by hand, those can be later introduced back into the system as new training data, representing points in the input space not learned by the network. The accuracy together with the efficiency against the threshold are shown in figure \ref{fig:InceptionACC_EFF_THETA}. A reasonable choice for this threshold is $\theta=0.7$, resulting in an accuracy of $95\%$ classifying $92\%$ of all damage sites. 
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{Inception_ACC_CLA_THETA.pdf}
%\caption{Accuracy of classified sites plotted together with the ratio of classified sites against the threshold. At $\theta=0.7$ of the $92\%$ classified damage sites $95\%$ were classified correctly.}
%\label{fig:InceptionACC_EFF_THETA}
%\end{figure}
%
%%Due to the first network acting as a filter for inclusions, it is necessary to minimize the number of damage sites falsely classified as inclusions or equivalently minimizing the number of false negatives. The relevant quantity for inspecting the performance under these conditions of a binary classifier, in this case the neural network, is its precision (positive predictive value) defined by
%%\begin{equation}
%%PPV = \frac{TP}{TP+FP}
%%\end{equation}
%%where $TP$ is the number of correctly classified inclusions and $FP$ is the number of other damage sites classified as inclusions. \\
%%On the other hand for the network to be useful, the number of correctly classified inclusions should be maximized, in order to reduce the amount of work necessary to relabel remaining damage sites by hand. 
%
%%\subsection{Ex-situ to in-situ}
%%While the first classifier, responsible for filtering out the inclusion sites, performed well on the ex-situ data sets, problems arose while trying to use it for the classification of in-situ damage sites. By using some of the in-situ data for the training of the network, its performance was substantially increased. The data sets for training and testing of the classifier are shown in the following table. \\
%%
%%\begin{tabular}{| l | c | c | c | c |}
%%\hline
%% & ex-situ train & ex-situ test & in-situ train & in-situ test \\ \hline
%%excluding in-situ & training & training & ignored & testing \\ \hline
%%including in-situ & training & training & training & testing \\ \hline
%%only ex-situ data & training & testing & ignored & ignored \\ \hline
%%\end{tabular}
%%
%%\subsubsection{Training excluding in-situ data}
%%Without including the in-situ data in the training set of the network, it characterized $52$ out of the $62$ inclusion sites correctly, while also classifying $164$ out $409$ sites that aren't inclusions as inclusions. Since the purpose of the first network is to filter out inclusions, this performance would make it inapplicable. While some of the inclusion sites not labeled as inclusions, can pass through the system and have to be labeled afterwards by hand, labeling sites that aren't inclusions with a high confidence as inclusions poses a huge problem for its usage as a filtering system. 
%
%%\subsubsection{Training including in-situ data}
%%By including some of the in-situ data into the training set, $44$ out of the $62$ inclusions sites were classified correctly as inclusions, performing slightly worse than the network trained on only the ex-situ data set. However none of the remaining $409$ sites were classified as inclusions.
%
%%\subsubsection{Training including all data}
%%Due to new data being created, one more test was included. The performance of the network trained on the final dataset is shown in figure \ref{fig:FirstClassifierFinal}. As can be seen the performance of the network 
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{FinalPerformanceInception.pdf}
%\caption{Purity plotted against efficiency of the network trained on the full dataset. As a comparison the performance of randomly assigning categories is shown as well.}
%\label{fig:FirstClassifierFinal}
%\end{figure}
%
%%\subsubsection{Comparison}
%%In figure \ref{fig:TPR_comparison} the precision of both differently trained networks are shown. As one can see the network performed well, while working only with ex-situ data. Transferring the network to be used on in-situ data the networks precision dropped immensely, rendering the network useless for classifying in-situ images. However, by including a small portion of in-situ data in the training of the network, the number of damage sites wrongly classified by the network as inclusions becomes negligible above a certain threshold. 
%
%\subsection{Confusion Matrix}
%
%
%\newpage
%\section{Second Stage}
%Due to the increased number of labeled damage sites, a more complex network, the InceptionV3 network, was tested in order to assess whether the accuracy can be increased by using a different architecture. The comparison between the two networks can be seen in figure \ref{fig:InVsEE}. Eventhough the amount of data was increased significantly, the simpler architecture still performs better than the complex one. 
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{InceptionVsEERACN_all.pdf}
%\caption{Performance comparison between the InceptionV3 network and the EERACN network distinguishing between brittle and ductile mechanisms}
%\label{fig:InVsEE}
%\end{figure}
%
%%\begin{figure}
%%  \includegraphics[width=\linewidth]{InceptionVsEERACN_nothing.pdf}
%%\caption{Accuracy on detected shadows found by the localization algorithm}
%%\label{fig:TPR_comparison}
%%\end{figure}
%
%\subsection{Brittle versus Ductile Damage Mechanisms}
%Furthermore a further split of the class hierarchy was considered. Instead of distinguishing between the three remaining classes, martensite cracking, interface decohesion, and notch effect, the second classifier should distinguish between brittle and ductile damage mechanisms. Therefore interface decohesions and notch effects are grouped into one class. In figure \ref{fig:2vs3Classes} the true positive rate can be seen 
%Furthermore we tested whether it is sensible to train a network capable of only distinguishing between brittle damage mechanisms (Martensite cracking) and ductile damage mechanisms (interface decohesion and notch effects) involved in the formation of voids. As can be seen in figure \ref{fig:2vs3Classes} a network trained to distinguish ductile damage mechanisms in interface decohesion and notch effects performs just as well if not better in distinguishing between brittle and ductile damage mechanisms as a network trained just for that task.
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{EERACN_2vs3Classes.pdf}
%\caption{True positive rates for the EERACN network distinguishing between two classes (Martensite and rest) and between three classes (Martensite, interface decohesion, and notch)}
%\label{fig:2vs3Classes}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{EERACN_differentTrainingSets.pdf}
%\caption{Accuracy plotted against the classification rate for the EERACN network trained on different training sets evaluated on all test sets}
%\label{fig:TPR_comparison}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{EERACN_differentTrainingSets_test_in_situ.pdf}
%\caption{Accuracy plotted against the classification rate for the EERACN network trained on different training sets evaluated on the original in-situ test set}
%\label{fig:TPR_comparison}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{EERACN_differentTrainingSets_test_Stufe0.pdf}
%\caption{Accuracy plotted against the classification rate for the EERACN network trained on different training sets evaluated on the stage zero test set}
%\label{fig:TPR_comparison}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{EERACN_differentTrainingSets_test_Deformed.pdf}
%\caption{Accuracy plotted against the classification rate for the EERACN network trained on different training sets evaluated on the deformed test set}
%\label{fig:TPR_comparison}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{PPV_different_classes_ex_situ.pdf}
%\caption{PPV for all classes trained on ex-situ data}
%\label{fig:TPR_comparison}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{PPV_different_classes_in_situ.pdf}
%\caption{PPV for all classes trained on in-situ data}
%\label{fig:TPR_comparison}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{PPV_different_classes_stage0.pdf}
%\caption{PPV for all classes trained on stage zero data}
%\label{fig:TPR_comparison}
%\end{figure}
%
%\begin{figure}
%  \includegraphics[width=\linewidth]{PPV_different_classes_deformed.pdf}
%\caption{PPV for all classes trained on deformed data }
%\label{fig:TPR_comparison}
%\end{figure}
%
%
\section{Combined Classifier}

\subsection{In-situ Analysis with Position Tracking}

Combining both classifiers


The combined classifier was used in order to investigate the time evolution of damage sites. This is done by firstly cropping the panoramas in such a way that at the borders of the images at different stages of stress the same physical features can be seen. For the first few stages the naive approach to just using the coordinates $x,y$ of a damage site in stage $i$ and rescaling them by the 


















